{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjnzZ0dM9Yc1"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate peft bitsandbytes sentencepiece sacrebleu evaluate huggingface_hub datasets fsspec\n",
        "import torch, random, os, json, itertools, textwrap\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import evaluate, tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZtzNmdTCxZsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6_ZfQAz8Q8W"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRHvol5u9iHB"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"meta-llama/Llama-3.2-1B\"\n",
        "SRC_LANG, TGT_LANG = \"English\", \"Spanish\"\n",
        "# LoRA hyper-params\n",
        "lora_conf = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "BATCH = 4\n",
        "EPOCHS = 2\n",
        "MAX_LEN = 256\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_conf = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                              bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL, device_map=\"auto\", quantization_config=bnb_conf)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_conf)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "DYn35UwABHqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(text, src=SRC_LANG, tgt=TGT_LANG):\n",
        "    return (f\"Translate the following text from {src} to {tgt}:\\n\"\n",
        "            f\"{src}: {text}\\n\"\n",
        "            f\"{tgt}:\")"
      ],
      "metadata": {
        "id": "dMjWbupxE7DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "# ---------- tiny slice just for demo ----------\n",
        "subset = load_dataset(\"opus_books\", \"en-es\", split=\"train[:3%]\")\n",
        "temp   = subset.train_test_split(test_size=0.20, seed=42)\n",
        "train_raw, test_raw = temp[\"train\"], temp[\"test\"]\n",
        "tmp    = train_raw.train_test_split(test_size=0.20, seed=42)\n",
        "train_raw, valid_raw = tmp[\"train\"], tmp[\"test\"]\n",
        "\n",
        "ds = DatasetDict(train=train_raw, validation=valid_raw, test=test_raw)\n",
        "\n",
        "def preprocess(ex):\n",
        "    src, tgt = ex[\"translation\"][\"en\"], ex[\"translation\"][\"es\"]\n",
        "\n",
        "    # 1Ô∏è‚É£ build full prompt **plus** answer in one sequence\n",
        "    full_text = format_prompt(src) + \" \" + tgt + tokenizer.eos_token\n",
        "    enc = tokenizer(full_text, truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "    # 2Ô∏è‚É£ mask the prompt part with -100 so loss is only on the answer\n",
        "    prompt_ids = tokenizer(format_prompt(src), add_special_tokens=False)[\"input_ids\"]\n",
        "    labels = [-100] * len(prompt_ids) + enc[\"input_ids\"][len(prompt_ids):]\n",
        "\n",
        "    assert len(labels) == len(enc[\"input_ids\"]), \"label/ids length mismatch\"\n",
        "\n",
        "    enc[\"labels\"] = labels\n",
        "    enc[\"reference\"] = tgt          # keep plain text for BLEU\n",
        "    return enc\n",
        "\n",
        "tokenized = ds.map(preprocess, remove_columns=ds[\"train\"].column_names, num_proc=4)\n",
        "tokenized.set_format(type=\"torch\",\n",
        "                     columns=[\"input_ids\", \"labels\", \"attention_mask\", \"reference\"])"
      ],
      "metadata": {
        "id": "Hd0nMNHeFtfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPZB0vPAMO3Y"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./lora-llama32-translate\",\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    bf16=True,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  args=args,\n",
        "                  train_dataset=tokenized[\"train\"],\n",
        "                  eval_dataset=tokenized[\"validation\"],\n",
        "                  data_collator=data_collator)\n",
        "\n",
        "trainer.train()\n",
        "output_path = \"/content/drive/MyDrive/lora-llama32-en-es\"\n",
        "model.save_pretrained(output_path)\n",
        "tokenizer.save_pretrained(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "# Load base model + tokenizer\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16\n",
        ")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "base_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Load LoRA-adapted model (same base + adapter weights)\n",
        "tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16\n",
        ")\n",
        "tuned_model = PeftModel.from_pretrained(tuned_model, \"/content/drive/MyDrive/lora-llama32-en-es\")\n",
        "\n",
        "tuned_tokenizer = base_tokenizer  # same tokenizer"
      ],
      "metadata": {
        "id": "Akj3X-h6bjxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, tokenizer, prompt, max_new_tokens=64):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        do_sample=False\n",
        "    )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True).strip()"
      ],
      "metadata": {
        "id": "7YyYwpUNcdap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(text, src=\"English\", target=\"Spanish\"):\n",
        "    return f\"Translate this from {src} to {target}:\\n{src}: {text}\\n{target}:\"\n",
        "\n",
        "samples = [\n",
        "    \"I need to get braces for my overbite.\",\n",
        "    \"How long does the treatment usually take?\",\n",
        "    \"My dentist said I have a crossbite.\",\n",
        "    \"Will it hurt when I get my aligners?\",\n",
        "    \"I lost my last tray. What should I do?\"\n",
        "]\n",
        "\n",
        "for s in samples:\n",
        "    prompt = format_prompt(s)\n",
        "    print(f\"\\nüìå Input: {s}\")\n",
        "    print(\"üîπ Base Model:\", translate(base_model, base_tokenizer, prompt))\n",
        "    print(\"üî∏ Tuned Model:\", translate(tuned_model, tuned_tokenizer, prompt))"
      ],
      "metadata": {
        "id": "P6HqmLL6coAB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}